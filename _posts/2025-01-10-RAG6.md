---
title: Learning how to use prompts with ChatOllama.
description: Let's create a prompt manually and build LangChain with ChatOllama.
author: DS2Man
date: 2025-01-07 11:00:00 +0000
categories:
  - Journey into Generative AI
tags:
  - RAG
  - ChatOllama
  - LangChain
  - LCEL
  - Prompt
math: true
pin: true
---

In the previous post, I used `ChatBrainAI()` to apply various general prompts and conversational prompts to configure LangChain. Going forward, I plan to build all projects using **ChatOllama()**.   
In the this post, I aim to apply general prompts and conversational prompts to configure LangChain.  Additionally, both general and conversational prompts seem to work in the same format across most LLM models supported by Ollama. Thanks to optimization for supported models, the inference speed is notably fast.

<!--
이전 글에서는 ChatBrainAI()를 활용해서 다양한 일반형 Prompt와 대화형 Prompt를 적용, LangChain을 구성했었다.
앞으로는 ChatOllama()를 활용해서 모든 프로젝트를 구성하려고 한다. 
이번글에서는 일반형 Prompt와 대화형 Prompt를 적용, LangChain을 구성하고자 한다. 
추가로, 일반형/대화형 Prompt 모두 Ollama에서 지원하는 대부분의 LLM 모델에서 동일한 포멧으로 동작하는 것으로 보인다. 지원모델을 최적화한 이유로 추론 속도가 매우 빠른 편이다.
-->

## *Applying PromptTemplate*

- Testing Model : gemma2:latest, llama3.2-vision:latest, llava:7b, gemma:7b, deepseek-r1:14b

~~~python
from langchain_ollama import ChatOllama
from ai.local.langchain_brainai import stream_response, invoke_response
from langchain_core.prompts import PromptTemplate

selected_model = "gemma2:latest"
llm = ChatOllama(model=selected_model)

template = """
#System:
You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
#Question:
{question}
#Answer:
"""
# The format below also works.
# template = """You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
# {question}
# """

prompt = PromptTemplate.from_template(
    template
) 

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question":question})
# invoke_response(response, "")
~~~

```
The capital of the United States is Washington, D.C.
```

## *Applying ChatPromptTemplate*

- Testing Model : gemma2:latest, llama3.2-vision:latest, llava:7b, gemma:7b, deepseek-r1:14b

~~~python
from langchain_ollama import ChatOllama
from ai.local.langchain_brainai import stream_response, invoke_response
from langchain_core.prompts import ChatPromptTemplate

selected_model = "gemma2:latest"
llm = ChatOllama(model=selected_model)

template =  [
    ("system", "You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly."),
    ("human", "{question}")
]

prompt = ChatPromptTemplate.from_messages(
    template
)

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, "")
~~~

```
The capital of the United States is Washington, D.C.
```