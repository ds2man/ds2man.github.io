---
title: LangChain using My Own Custom class, ChatBrainAI.
description: Let's write code to create a LangChain using My Own Custom class,ChatBrainAI
author: DS2Man
date: 2025-01-06 11:00:00 +0000
categories:
  - Journey into Generative AI
tags:
  - RAG
  - ChatBrainAI
  - LangChain
  - LCEL
math: true
pin: true
---

In the previous post, we explored how to configure LangChain using Hugging Face Transformers. I created my own custom class that can be used similarly to the ChatOpenAI() class. I hope it proves to be useful wherever needed. If there are any issues or corrections, please feel free to reach out. 

<!--
이전 포스트에서 우리는 Hugging face transformers를 활용해서 LangChain을 구성해봤다. 나는 ChatOpenAI() 클래스처럼 사용할 수 있도록 나만의 클래스를 만들어 보았다. 필요한 곳에 유용하게 사용되었으면 합니다. 혹시 잘못된 부분이 있다면 연락주세요.
-->

## *My Own Custom class, ChatBrainAI*

~~~python
from abc import ABCMeta
from abc import abstractmethod  

class AIModelBase(metaclass=ABCMeta):
    @abstractmethod
    def __init__(self):
        pass

    @abstractmethod
    def generate(sef):
        pass
~~~

~~~python
class ChatBrainAI(AIModelBase):
    def __init__(
        self,
        model_name: str = "gemma-2-2b-it",
        temperature: float = 0.1,
    ):

        try:
            path = os.path.join(os.getcwd(), "Pretrained_byGit")
            self.__model_name = os.path.join(path, model_name)
            self.__temperature = temperature
            # 1.Load the model and tokenizer using transformers
            self.__tokenizer = AutoTokenizer.from_pretrained(self.__model_name)
            self.__model = AutoModelForCausalLM.from_pretrained(
                self.__model_name, device_map="auto"
            )

            # Some parameter settings are required depending on the local model.
            if self.__tokenizer.pad_token_id is None:
                self.__tokenizer.pad_token_id = 0
                print(f"Pad token ID is set to: {self.__tokenizer.pad_token_id}")
            else:
                print(f"Pad token ID already set: {self.__tokenizer.pad_token_id}")
            # 2.Configure the pipeline
            self.__pipe = pipeline(
                "text-generation"
                model=self.__model,
                tokenizer=self.__tokenizer,
                max_new_tokens=512,
                do_sample=True,
                temperature=self.__temperature,
                pad_token_id=self.__tokenizer.pad_token_id,
            ) 
            # 3.Create a HuggingFacePipeline object for LangChain
            self.__pipeline = HuggingFacePipeline(pipeline=self.__pipe)

        except Exception as e:
            print(f"Error occurred while creating LangChain:\n{e}")
            self.__pipeline = None

    def generate(self, prompt: str):
        if self.__pipeline is None:
            return None

        try:
            response = self.__pipeline
            return response
        except Exception as e:
            print(f"Error occurred: {e}")
            return None
            
    # Use the object like a function, similar to the ChatOpenAI()
    def __call__(self, prompt: str):        
        return self.generate(prompt)
~~~

## *Methods for output when called via invoke() and stream()*

1. **invoke_response**

~~~python
from langchain_core.messages import AIMessageChunk, AIMessage

def invoke_response(response, suffix="", return_output=False):
    if suffix != "":
        answer = response.split(suffix)[-1].strip()
        print(answer)
    else:
        if isinstance(response, AIMessage):
            # langchain_core.messages.ai.AIMessage
            answer = response.content
            print(answer)
        elif isinstance(response, str):
            answer = response
            print(answer)
        else:
            print(f"The type of response is ... {type(response)}...\n Please check!")

    if return_output:
        return answer

~~~

2. **stream_response**

~~~python
from langchain_core.messages import AIMessageChunk, AIMessage

def stream_response(response, return_output=False):
    answer = ""
    for chunk in response:
        if isinstance(chunk, AIMessageChunk):
            # langchain_core.messages.ai.AIMessageChunk
            answer += chunk.content
            print(chunk.content, end="", flush=True)
        elif isinstance(chunk, str):
            answer += chunk
            print(chunk, end="", flush=True)
        else:
            print(f"The type of chunk is ... {type(chunk)}...\n Please check!")

    if return_output:
        return answer
~~~


## *Example using ChatBrainAI()*

~~~python
from ai.local.langchain_brainai import ChatBrainAI, stream_response, invoke_response
from langchain_core.prompts import PromptTemplate

llm = ChatBrainAI('Llama-3.2-3B-Instruct')

template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
<|eot_id|><|start_header_id|>user<|end_header_id|>{question}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""

prompt = PromptTemplate.from_template(
    template
) 

chain = prompt | llm
~~~

~~~python
question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)
~~~

```
The capital of the United States is Washington, D.C.
```

~~~python
question = "What is the capital of the United States?"
response = chain.invoke({"question": question})
invoke_response(response, "<|end_header_id|>")
~~~

```
The capital of the United States is Washington, D.C.
```