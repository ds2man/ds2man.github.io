---
title: Differences in prompt formats based on LLM models.
description: Let's identify the differences in prompt formats based on LLM models.
author: DS2Man
date: 2025-01-09 11:00:00 +0000
categories:
  - Journey into Generative AI
tags:
  - RAG
  - ChatBrainAI
  - LangChain
  - LCEL
  - Prompt
math: true
pin: true
---

In this post, I present the results of experiments conducted using several models registered on Hugging Face, leveraging `ChatBrainAI()`.   
General prompts (`PromptTemplate`) have different formats depending on the model, meaning they need to be managed accordingly. I will compare the prompt formats of three models.  
Conversational prompts (`ChatPromptTemplate`) are categorized by roles (`system`, `user`, `assistant`), so the prompt formats for most models seem to be similar.  
To conclude, while the models respond well to `PromptTemplate`, the results for `ChatPromptTemplate` do not seem as favorable. It appears that non-conversational LLMs do not respond effectively. I plan to update this post further as I gain more experience in the future.

<!--
이번 글에서는 ChatBrainAI()을 활용해서 허깅페이스에 등록된 몇가지 모델을 가지고 실험한 결과입니다.
일반형 프롬프트(PromptTemplate)는 모델에 따라 포멧이 다릅니다. 즉 모델에 따라서 관리될 필요가 있습니다. 3모델에 대해서 Prompt Format을 비교해보겠습니다.
대화형 프롬프트(ChatPromptTemplate)는 Role 구분(system, user, assistant)되어 있습니다.  그래서 대부분의 모델의 Prompt Format이 유사한 거 같습니다.
결론적으로 이야기 하면 PromptTemplate에는 잘 응답하나, ChatPromptTemplate의 경우에는 답변의 결과가 좋은 것 같지 않습니다. 
대화형 LLM이 아닌 모델이 응답을 잘 안하는 거 같은데, 향후 경험이 쌓이면 해당 글은 업데이트를 추가적으로 할 예정입니다.
-->

## *Selecting Prompt Format Based on LLM Model*

Each model has its own corresponding template format, which needs to be recognized and addressed accordingly. The differences in prompts across three models are outlined below. As mentioned earlier, I plan to focus primarily on ChatOllama(). I will explain it in the next post. The LangChain setup is the same, so I will omit it.
 
<!--
모델에 맞는 Template Format이 존재함. 이것을 인지하고 대응해줘야합니다. 3가지 모델에 따른 Prompt 차이점은 아래와 같습니다. 앞서 말한것처럼 저는 ChatOllama() 위주로 사용할 예정입니다 (다음 글에서 설명할께요). LangChain 구성은 동일하므로 생략할께요.
-->

~~~python
llm = ChatBrainAI('gemma-2-2b-it')
template = """<start_of_turn>system
    You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
    <end_of_turn>
    <start_of_turn>user
    {question}
    <end_of_turn>
    <start_of_turn>model
"""
suffix = "<start_of_turn>model"

llm = ChatBrainAI('Llama-3.2-3B-Instruct')
template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
<|eot_id|><|start_header_id|>user<|end_header_id|>{question}
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
suffix = "<|end_header_id|>"

llm = ChatBrainAI('EEVE-Korean-Instruct-2.8B-v1.0')
template = """You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
Human: {question}
Assistant:
"""
suffix = "Assistant:"
~~~

~~~python
prompt = PromptTemplate.from_template(
    template
) 

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, suffix)
~~~

```
The capital of the United States is Washington, D.C.
```


## *Understanding ChatPromptTemplate*

It is a template primarily used for chat-based conversational models, with roles categorized as `system`, `user`, and `assistant`. In particular, prompts based on conversational content are mainly implemented using **ChatPromptTemplate.from_message()**. To conclude, while the models respond well to `PromptTemplate`, the results for `ChatPromptTemplate` do not seem as favorable. It appears that non-conversational LLMs do not respond effectively. 

|Model|Response|Notes|
|---|:---:|:---|
|gemma-2-2b-it|ㅇ|-|
|Llama-3.2-3B-Instruct|x|반복 질문을 내부적으로 계속함|
|EVE-Korean-Instruct-2.8B-v1.0|x|쓸데없는 답변이 많이 발생|

<!--
채팅 기반 대화형 모델에 주요 사용되는 템플릿입니다. Role 구분(system, user, assistant)되어 있습니다. 특히 대화 내용을 기반으로 한 Prompt 구현은 ChatPromptTemplate.from_message() 로 주로 합니다. 
-->

~~~python
from ai.local.langchain_brainai import ChatBrainAI, stream_response, invoke_response
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly."),
        ("human", "{question}\nAnswer:")
    ]
)

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, "Answer:")
~~~

```
The capital of the United States is Washington, D.C.
```