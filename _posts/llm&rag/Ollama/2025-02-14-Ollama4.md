---
title: Learning how to connect Ollama.
description: Let's connect Ollama in Server.
author: DS2Man
date: 2025-02-14 11:00:00 +0000
categories: [LLM&RAG, L&R-Ollama]
tags:
  - Ollama
math: true
pin: true
---

We have installed Ollama on both Windows and Ubuntu (WSL). However, if we want to provide an LLM service, Ollama needs to run on a server. To do that, we will likely need to run a Docker container in a Kubernetes (k8s) environment.  ~~(There's also the old-fashioned way of installing it directly on bare metal.)~~      
I'll write this post taking several scenarios into consideration.

<!--
우리는 ollama를 windows와 Ubuntu(WSL)에 설치해 보았다. 그런데 우리가 LLM 서비스를 하려면 Ollama가 서버에서 동작해야하고 그렇게 하려면 k8s환경에서 docker container를 run해서 진행해야할 것이다.~~(bare metal 에 직접 설치하는 옛날 방식도 있긴하다.)~~ 
몇가지 상황을 고려해서 글을 쓰도록 하겠다.
-->

- **(Case 1) Let’s pull the Ollama image and run the container on a specific server.**
- (Case 2) Automatically load a pre-specified LLM model when running the Ollama container on a specific server.
- (Case 3) Automatically load a pre-specified LLM model when running the Ollama container on a specific **isolated (air-gapped)** server.

<!--
다음과 같이 가졍하고 진행하겠다.
(상황1)특정 서버에서 운영한다고 가정하고 ollama image를 가져와서 container 를 실행해 보자.
-->

## *Confirmed that the Ollama image exists on Docker Hub*

![Ollama on Dockerhub](/assets/img/llm&rag/ollama/2025-02-14-Ollama4_1.png)
_Ollama on Dockerhub(Source: [Dockerhub](https://hub.docker.com/r/ollama/ollama/))_

You can find the `ollama/ollama` image available on Docker Hub.    

## *Run the Ollama Container*

Since we’re using an Nvidia GPU and have already set up the NVIDIA Container Toolkit environment ([NVIDIA Container Toolkit](/posts/Docker-Setup6)), all we need to do now is run the following command.

<!--
Dockerhub에 들어가면 ollama/ollama image가 존재하는 것을 확인할 수 있다. 
앞서 Nvidia GPU를 사용하고 있고, NVIDIA Container Tookit 환경구성까지 했기 때문에([NVIDIA Contrainer Toolkit](/posts/Docker-Setup6)), 아래 명령어만 실행하면 된다.
-->

- `docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama`    

```bash
# Using `-v` (volume), we mount `/root/.ollama` inside the Docker image to the `ollama` folder on Ubuntu.
# Using `-p` (port), we expose port `11434` from the container to the host.
# Note: If you check the tags of `ollama/ollama` on Docker Hub, you'll see that `ENV OLLAMA_HOST=0.0.0.0:11434` is set. 
# This is important — without this setting, Ollama will be accessible inside the container, but not from outside the container.
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
Unable to find image 'ollama/ollama:latest' locally
latest: Pulling from ollama/ollama
d9802f032d67: Pull complete
161508c220d5: Pull complete
a5fe86995597: Pull complete
dfe8fac24641: Pull complete
Digest: sha256:74a0929e1e082a09e4fdeef8594b8f4537f661366a04080e600c90ea9f712721
Status: Downloaded newer image for ollama/ollama:latest
2a42f842e4156469e6a4f8710c5344035db06c105bee63201881d1bd00875489
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ 

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker images
REPOSITORY            TAG                            IMAGE ID       CREATED        SIZE
ollama/ollama         latest                         a67447f85321   3 days ago     3.45GB

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker ps -s
CONTAINER ID   IMAGE                                      COMMAND                  CREATED       STATUS                 PORTS                                                                                      NAMES               SIZE
2a42f842e415   ollama/ollama                              "/bin/ollama serve"      8 hours ago   Up 7 hours             0.0.0.0:11434->11434/tcp, :::11434->11434/tcp                                              ollama              11.4kB (virtual 3.45GB)
```

Let’s check the location of the Docker volume.  
For more information on how to use Docker volumes, please refer to the link below:  
[Container Volume](/posts/Docker-Understanding6)

<!--
docker volume 위치를 파악해보자.
docker volume 사용법은 아래 링크를 참고바란다.
[NVIDIA Contrainer Toolkit](/posts/Docker-Understanding6)
-->

```bash
# The Docker volume is named `ollama`, and its location is `/var/lib/docker/volumes/ollama/_data`.
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker volume ls
DRIVER    VOLUME NAME
local     ollama

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker volume inspect ollama
[
    {
        "CreatedAt": "2025-03-21T21:47:36+09:00",
        "Driver": "local",
        "Labels": null,
        "Mountpoint": "/var/lib/docker/volumes/ollama/_data",
        "Name": "ollama",
        "Options": null,
        "Scope": "local"
    }
]

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ su
Password:
root@DESKTOP-B7GM3C5:/home/jaoneol/dcai/test-ollama# cd /var/lib/docker/volumes/ollama/_data
root@DESKTOP-B7GM3C5:/var/lib/docker/volumes/ollama/_data# ls -lrt
total 16
-rw-r--r-- 1 root root   81 Mar 21 21:47 id_ed25519.pub
-rw------- 1 root root  387 Mar 21 21:47 id_ed25519
-rw------- 1 root root    7 Mar 21 22:13 history
drwxr-xr-x 3 root root 4096 Mar 22 05:22 models
root@DESKTOP-B7GM3C5:/var/lib/docker/volumes/ollama/_data#
```

## *Test the Ollama Container*

Ollama has a REST API for running and managing models.  
You can easily test it out.

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt":"What is the capital of America?"
}'
```

When you run it, you’ll see a message saying that `gemma3:latest` is not available.  
Let’s pull `gemma3:latest` by running the following command.

- `docker exec -it ollama /bin/bash`   
-  `ollama pull gemma3`

```bash
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt":"What is the capital of America?"
}'
{"error":"model 'gemma3' not found"

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker exec -it ollama /bin/bash
root@7f9deabefe82:/# ollama pull gemma3
pulling manifest
pulling 377655e65351... 100% ▕████████████████████████████████████▏ 3.3 GB
pulling e0a42594d802... 100% ▕████████████████████████████████████▏  358 B
pulling dd084c7d92a3... 100% ▕████████████████████████████████████▏ 8.4 KB
pulling 0a74a8735bf3... 100% ▕████████████████████████████████████▏   55 B
pulling ffae984acbea... 100% ▕████████████████████████████████████▏  489 B
verifying sha256 digest
writing manifest
success
root@7f9deabefe82:/# ollama list
NAME             ID              SIZE      MODIFIED
gemma3:latest    c0494fe00251    3.3 GB    6 seconds ago
root@7f9deabefe82:/# exit
exit
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ curl http://localhost:11434/api/generate -d '{
  "model": "gemma3",
  "prompt":"What is the capital of America?"
}'
{"model":"gemma3","created_at":"2025-03-21T20:52:02.874631088Z","response":"The","done":false}
......
{"model":"gemma3","created_at":"2025-03-21T20:52:03.659462344Z","response":"?","done":false}
{"model":"gemma3","created_at":"2025-03-21T20:52:03.670390259Z","response":"","done":true,"done_reason":"stop","context":[105,2364,107,3689,563,506,5279,529,5711,236881,106,107,105,4368,107,818,5279,529,506,3640,4184,529,5711,563,5213,42804,236764,622,236761,236780,99382,236743,108,1509,11979,573,9252,529,18693,236761,103453,236743,108,6294,611,1461,531,1281,4658,919,1003,7693,236764,622,236761,236780,236761,653,506,749,236761,236773,236761,3251,236881],"total_duration":8892756097,"load_duration":7656020945,"prompt_eval_count":16,"prompt_eval_duration":439450067,"eval_count":51,"eval_duration":796620275}
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$
```

In the previous section, we accessed the container and manually executed the `pull` command to load the model into Ollama. However, if you're running this on a server, you'll face the issue of needing to do this every time the container restarts.  In the next post, we’ll explore how to automatically load a pre-specified model.    
For reference, it’s also possible to execute the `ollama pull` command using Python — just for your information.
<!--
위에서는 Ollama에 model을 가져오기 위해서 container에 접속해서 pull 명령어를 직접 실행했다.
만약 서버에서 운영된다면 container 재실행될때마다 봐야하는 문제가 있다.
다음 글에서는 미리 지정한 model을 가져오는 방법을 알아볼 것이다.

참고로 python으로 ollama pull 명령을 할수 있는 방안도 있는데, 참고만 하자.
-->

```python
import subprocess
from langchain_ollama import ChatOllama
from ollama import Client
from ollama._types import ResponseError

model_name = 'gemma3:1b'

# 모델 존재 여부 확인 및 필요시 pull
def ensure_model_exists(model: str):
    client = Client()
    try:
        models_info = client.list().get("models", [])
        model_tags = [m.get("tag") for m in models_info if "tag" in m]
        if model not in model_tags:
            print(f'Model "{model}" not found locally. Pulling it...')
            subprocess.run(["ollama", "pull", model], check=True)
    except Exception as e:
        print(f"모델 확인 또는 pull 중 오류 발생: {e}")
        raise

# 모델 확인 및 가져오기
ensure_model_exists(model_name)

# LLM 사용
llm = ChatOllama(model=model_name, url="http://localhost:11434")
response = llm.invoke("AI의 미래에 대해 100자 내외로 알려줘")
print(response)
```