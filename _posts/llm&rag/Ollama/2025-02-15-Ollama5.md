---
title: Setup Custom Ollama on a specific server.
description: Let's setup Custom Ollama Container on a specific server.
author: DS2Man
date: 2025-02-15 11:00:00 +0000
categories: [LLM&RAG, L&R-Ollama]
tags:
  - Ollama
math: true
pin: true
---

In this post, I'll focus on the second scenario.

- [(Case 1) Let’s pull the Ollama image and run the container on a specific server.](/posts/Ollama4)
- _**(Case 2) Automatically load a pre-specified LLM model when running the Ollama container on a specific server.**_
- (Case 3) Automatically load a pre-specified LLM model when running the Ollama container on a specific **isolated (air-gapped)** server.

## *Select base image on Docker Hub*

The first step when building a Docker image is selecting the base image. Since our goal is to run Ollama, GPU support is required. So, let’s first check the CUDA version of the specific server.   
For now, my PC has CUDA version 12.6, so let’s assume that the specific server also has CUDA 12.6 installed.

<!--
docker image 만들때 맨 처음 하는 거는 base image를 선정하는 것이다. 우리는 ollama를 실행 시킬 목적이므로 gpu가 구동되어야한다. 그래서 specific server의 cuda 버전을 확인해 보도록하자.
우선 내 PC의 CUDA 버전은 12.6이므로 specific server의 cuda 버전도 동일하게 12.6이라고 가정하자.
-->

After searching on Docker Hub, the base image I selected is `nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04`. Here are the reasons for this choice:    
-  `nvidia/cuda:12.6.3-cudnn` : My PC uses CUDA 12.6
- `runtime`:   
     `runtime` includes only the libraries needed to run CUDA programs → smaller image size.  
     Since we're only running Ollama, the `runtime` version is more suitable!   
     `devel` includes development tools such as compilers → use this when model training is required.       
- `buntu22.04`: The most stable Ubuntu version.
  
<!--
DOCKER HUB에서 검색후 내가 선정한 BASE IMAGE는  `nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04`이다.  다음과 같은 이유로 선정했다.
- `nvidia/cuda:12.6.3-cudnn` : 나의 PC의 CUDA는 12.6 
- runtime
  > runtime : CUDA 프로그램을 실행하는 데 필요한 라이브러리만 포함 → 이미지 크기가 작음.  Ollama를 실행하기만 한다면 -runtime 버전이 더 적합!
  > devel : 컴파일러 등 개발 도구 포함 → 모델 학습이 필요한 경우 사용.
- ubuntu22.04 : 가장 안정적인 ubunut 버전
-->

## *Make dockerfile*

```bash
FROM nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04

# Set non-interactive mode for apt-get
ENV DEBIAN_FRONTEND=noninteractive

# Install required dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    rsync \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Set up runtime for NVIDIA support
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Note. Change Ollama Host. If you don't change, you don't connect ollama outside ollama container.
ENV OLLAMA_HOST=0.0.0.0:11434

# Copy entrypoint script
COPY entrypoint_imake-ollama.sh /entrypoint_imake-ollama.sh
RUN chmod +x /entrypoint_imake-ollama.sh

ENTRYPOINT ["/entrypoint_imake-ollama.sh"]
```

## *Make entrypoint file*

In a Dockerfile, `ENTRYPOINT` or `CMD` can only be executed once when the container starts ([Dockerfile Commands](/posts/Docker-Understanding4)). If you want to run multiple commands, you need to create and execute a shell script.     
In our case, we need to run two commands.   
- ollama serve
- download llm model

<!--
dockerfile에서 entrypoint 또는 CMD는 컨테이너가 실행될 때 한번만 실행이 가능하다.([Dockerfile Commands](/posts/Docker-Understanding4)) 만약에 어려개의 명령어를 사용하고 싶으면 쉘 스크립트를 만들어서 실행해야 한다. 우리는 두가지 명령어가 필요하다.
- ollama serve
- download llm model
-->

```bash
#!/bin/bash

ollama serve &
# Wait for the server to start
sleep 10

# List of models to download
echo "Check defined model..."
MODELS=("gemma3:1b")

# Loop through each model and download if not already present
for model in "${MODELS[@]}"; do
    if ! ollama list | grep -q "$model"; then
        echo "Downloading $model model..."
        ollama pull "$model"
    else
        echo "$model model is already downloaded."
    fi
done

# Keep the Ollama server running
wait -n
```

## *Build the Ollama Image*

- `docker build -t ollama-gpu .`

```bash
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$ docker build -t ollama-gpu .

[+] Building 56.9s (10/10) FINISHED                                                                                                                                       docker:default

 => [internal] load build definition from dockerfile                                                                                                                                0.0s

 => => transferring dockerfile: 2.42kB                                                                                                                                              0.0s

 => [internal] load metadata for docker.io/nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04                                                                                             0.7s

 => [internal] load .dockerignore                                                                                                                                                   0.0s

 => => transferring context: 2B                                                                                                                                                     0.0s

 => CACHED [1/5] FROM docker.io/nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04@sha256:5f0d2d827f6436b3cb7468fd8acbdc8c1d41261614e579ae49afe6141da51133                                0.0s

 => [internal] load build context                                                                                                                                                   0.0s

 => => transferring context: 48B                                                                                                                                                    0.0s

 => [2/5] RUN apt-get update && apt-get install -y     curl     wget     rsync     ca-certificates     && rm -rf /var/lib/apt/lists/*                                              14.1s

 => [3/5] RUN curl -fsSL https://ollama.com/install.sh | sh                                                                                                                        38.1s

 => [4/5] COPY entrypoint_imake-ollama.sh /entrypoint_imake-ollama.sh                                                                                                               0.0s

 => [5/5] RUN chmod +x /entrypoint_imake-ollama.sh                                                                                                                                  0.2s

 => exporting to image                                                                                                                                                              3.5s

 => => exporting layers                                                                                                                                                             3.5s

 => => writing image sha256:7b76567481ca3f3351ed2451ccf0c6d71d03c687c0330eebf8f5bd5d1e63e1f0                                                                                        0.0s

 => => naming to docker.io/library/ollama-gpu                                                                                                                                       0.0s

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$ docker images

REPOSITORY            TAG                            IMAGE ID       CREATED          SIZE

ollama-gpu            latest                         7b76567481ca   41 seconds ago   6.64GB
```

## *Run the Ollama Container*

- `docker run -d --gpus all -v ollama_models:/root/.ollama/ -p 11434:11434 --name ollama-gpu ollama-gpu`

```bash
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$ docker run -d --gpus all -v ollama_models:/root/.ollama/ -p 11434:11434 --name ollama-gpu ollama-gpu

dd4b40438dd07ca86426dbc7c6643a561cbe2aea27751c3fcbbf2a525ebf316c

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/test-ollama$ docker ps -s
CONTAINER ID   IMAGE                                      COMMAND                  CREATED          STATUS                  PORTS                                                                                      NAMES               SIZE
89697f4cc367   ollama-gpu                                 "/entrypoint_imake-o…"   25 minutes ago   Up 25 minutes           0.0.0.0:11434->11434/tcp, :::11434->11434/tcp                                              ollama-gpu          20.2kB (virtual 6.64GB)

```

After running the container, let’s check the following:    
- Whether the GPU is being used   
  `docker exec -it ollama-gpu nvidia-smi`
- Whether `gemma3:1b` has been successfully pulled    
  `docker exec -it ollama-gpu ollama list`
- The Docker volume contents    
  `docker volume inspect ollama_models`

```bash
(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$ docker exec -it ollama-gpu nvidia-smi

Fri Mar 21 22:46:31 2025

+-----------------------------------------------------------------------------------------+

| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |

|-----------------------------------------+------------------------+----------------------+

| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |

| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |

|                                         |                        |               MIG M. |

|=========================================+========================+======================|

|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:01:00.0  On |                  N/A |

|  0%   39C    P8              9W /  220W |    1928MiB /  12282MiB |      0%      Default |

|                                         |                        |                  N/A |

+-----------------------------------------+------------------------+----------------------+

  

+-----------------------------------------------------------------------------------------+

| Processes:                                                                              |

|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |

|        ID   ID                                                               Usage      |

|=========================================================================================|

|    0   N/A  N/A         7      C   /ollama                                     N/A      |

|    0   N/A  N/A         8      C   /milvus                                     N/A      |

|    0   N/A  N/A        31      G   /Xwayland                                   N/A      |

|    0   N/A  N/A        73      C   /ollama                                     N/A      |

+-----------------------------------------------------------------------------------------+

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$ docker exec -it ollama-gpu ollama list

NAME         ID              SIZE      MODIFIED

gemma3:1b    2d27a774bc62    815 MB    5 minutes ago

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$ docker volume inspect ollama_models

[

    {

        "CreatedAt": "2025-03-22T07:33:30+09:00",

        "Driver": "local",

        "Labels": null,

        "Mountpoint": "/var/lib/docker/volumes/ollama_models/_data",

        "Name": "ollama_models",

        "Options": null,

        "Scope": "local"

    }

]

(base) jaoneol@DESKTOP-B7GM3C5:~/dcai/imake-ollama$
```

## *Test the Ollama Container*

Ollama has a REST API for running and managing models.  
You can easily test it out.

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "gemma3:1b",
  "prompt":"What is the capital of America?"
}'
```

We’ve confirmed that the pre-specified model is successfully loaded when running on a server. However, if the server is operating on a specific **isolated (air-gapped)** server, it won’t be able to fetch the model using `ollama pull`.    
In the next post, we’ll show how to load the pre-specified model from a MinIO instance that’s been set up on a specific **isolated (air-gapped)** server.
<!--
서버에서 운영할 때 미리 지정한 model을 정상적으로 가져오는 것을 확인해 봤다. 그런데 만약 폐쇄망에서 운영되는 서버일 경우 직접 ollama pull을 통해서 모델을 가져올 수 없다. 
다음 글에서는 미리 지정한 model을 폐쇄망의 Minio에 등록된 모델을 통해서 미리 지정한 model을 가져오게 하겠다.
-->
