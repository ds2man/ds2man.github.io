---
title: Learning how to connect Ollama.
description: Let's connect Ollama in Server.
author: DS2Man
date: 2025-02-10 11:00:00 +0000
categories: [LLM&RAG, L&R-Ollama]
tags:
  - Ollama
math: true
pin: true
---

## *How to connect Ollama on a server*

Ollama has been briefly mentioned in previous posts.     
[LangChain using ChatOllama](/posts/RAG3)    
[Learning how to use prompts with ChatOllama](/posts/RAG7)    
<br>
As explained in [Learning how to use prompts with ChatOllama](/posts/RAG7), let’s take another look at how to use Ollama. What if Ollama is set up on a server instead of your local PC? How should you connect to it? It’s very simple.     
<ins>When calling ChatOllama, just provide the server address where Ollama is running as the `base_url`.</ins>

- `ChatOllama(model="gemma2:latest")`    
     `default base_url : http://localhost:11434, http://127.0.0.1:11434`
- `ChatOllama(model="gemma2:latest", base_url="http://1.2.3.4:11434")`

### *Applying PromptTemplate*

- [Learning how to use prompts with ChatOllama](/posts/RAG7) in previous posts.     

~~~python
from langchain_ollama import ChatOllama
from ai.local.langchain_brainai import stream_response, invoke_response
from langchain_core.prompts import PromptTemplate

selected_model = "gemma2:latest"
# Note. base_url
llm = ChatOllama(model=selected_model, base_url="http://1.2.3.4:11434")

template = """
#System:
You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
#Question:
{question}
#Answer:
"""
# The format below also works.
# template = """You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
# {question}
# """

prompt = PromptTemplate.from_template(
    template
) 

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question":question})
# invoke_response(response, "")
~~~

```
The capital of the United States is Washington, D.C.
```

### *Applying ChatPromptTemplate*

- [Learning how to use prompts with ChatOllama](/posts/RAG7) in previous posts.     

~~~python
from langchain_ollama import ChatOllama
from ai.local.langchain_brainai import stream_response, invoke_response
from langchain_core.prompts import ChatPromptTemplate

selected_model = "gemma2:latest"
llm = ChatOllama(model=selected_model, base_url="http://1.2.3.4:11434")

template =  [
    ("system", "You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly."),
    ("human", "{question}")
]

prompt = ChatPromptTemplate.from_messages(
    template
)

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, "")
~~~

```
The capital of the United States is Washington, D.C.
```