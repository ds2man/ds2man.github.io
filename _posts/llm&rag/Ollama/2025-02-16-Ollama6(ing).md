---
title: Setup Custom Ollama on a specific isolated (air-gapped) server.
description: Let's setup Custom Ollama Container on a specific isolated (air-gapped) server.
author: DS2Man
date: 2025-02-16 11:00:00 +0000
categories: [LLM&RAG, L&R-Ollama]
tags:
  - Ollama
math: true
pin: true
---

In this post, I'll focus on the third scenario.

- [(Case 1) Let’s pull the Ollama image and run the container on a specific server.](/posts/Ollama4)
- [(Case 2) Automatically load a pre-specified LLM model when running the Ollama container on a specific server.](/posts/Ollama5)
- _**(Case 3) Automatically load a pre-specified LLM model when running the Ollama container on a specific **isolated (air-gapped)** server.**_


## *Select base image on Docker Hub*

The first step when building a Docker image is selecting the base image. Since our goal is to run Ollama, GPU support is required. So, let’s first check the CUDA version of the specific server.   
For now, my PC has CUDA version 12.6, so let’s assume that the specific server also has CUDA 12.6 installed.

After searching on Docker Hub, the base image I selected is `nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04`. Here are the reasons for this choice:    
-  `nvidia/cuda:12.6.3-cudnn` : My PC uses CUDA 12.6
- `runtime`:   
     `runtime` includes only the libraries needed to run CUDA programs → smaller image size.  
     Since we're only running Ollama, the `runtime` version is more suitable!   
     `devel` includes development tools such as compilers → use this when model training is required.       
- `buntu22.04`: The most stable Ubuntu version.

## *Make dockerfile*

```bash
FROM nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04

# Set non-interactive mode for apt-get
ENV DEBIAN_FRONTEND=noninteractive

# Install required dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    rsync \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Set up runtime for NVIDIA support
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Note. Change Ollama Host. If you don't change, you don't connect ollama outside ollama container.
ENV OLLAMA_HOST=0.0.0.0:11434

# Copy entrypoint script
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
```

## *Make entrypoint file*

In a Dockerfile, `ENTRYPOINT` or `CMD` can only be executed once when the container starts ([Dockerfile Commands](/posts/Docker-Understanding4)). If you want to run multiple commands, you need to create and execute a shell script.     
In our case, we need to run two commands.   
- ollama serve
- download llm model

```bash
#!/bin/bash

# Start Ollama server in the background
# OLLAMA_PORT=0.0.0.0:9249 is defined in the Dockerfile ENV.
ollama serve &

# Wait for the server to start
sleep 10

# List of models to download
echo "Check defined model..."
# MODELS=("gemma3:latest" "gemma3:12b")
MODELS=("gemma3:latest")

# MinIO server info
# Need to find a way to handle private buckets...!!!!
MINIO_URL="localhost:9000"  # 127.0.0.1:9000, localhost:9000 vs milvus-minio:9000
BUCKET="ds2man"
MODEL_PATH="ollama_models"
DEST_DIR="/root/.ollama/models"

# Temporary storage directory
TEMP_DIR="temp_models"
mkdir -p "$TEMP_DIR"

# Download, extract, and move models
for model in "${MODELS[@]}"; do
    if ollama list | grep -q "$model"; then
        echo "Model $model is already available. Skipping download."
        continue
    fi

    # Convert model name (e.g., gemma3:latest -> gemma3-latest)
    model_file=$(echo "$model" | tr ':' '-')
    tar_file="$model_file.tar.gz"

    # Download URL
    url="$MINIO_URL/$BUCKET/$MODEL_PATH/$tar_file"

    echo "Downloading $tar_file from $url"
    wget -q "$url" -O "$TEMP_DIR/$tar_file"

    if [[ $? -ne 0 ]]; then
        echo "Failed to download $tar_file"
        continue
    fi

    echo "Extracting $tar_file"
    mkdir -p "$TEMP_DIR/models"
    tar -xzvf "$TEMP_DIR/$tar_file" -C "$TEMP_DIR/models" --strip-components=1

    echo "Moving extracted files to $DEST_DIR"
    rsync -a "$TEMP_DIR/models/" "$DEST_DIR"

    # Clean up
    rm -rf "$TEMP_DIR/models"
    rm "$TEMP_DIR/$tar_file"
done

# Remove temporary directory
rmdir "$TEMP_DIR"  

echo "All models processed successfully." 

# Keep the Ollama server running
wait -n
```