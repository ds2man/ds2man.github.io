---
title: Learning how to use prompts.
description: Let's create a prompt manually and build LangChain. 
author: DS2Man
date: 2025-01-08 11:00:00 +0000
categories:
  - Journey into Generative AI
tags:
  - RAG
  - ChatBrainAI
  - LangChain
  - LCEL
  - Prompt
math: true
pin: true
---

Previous posts explained only using **PromptTemplate.from_template()**.   
The prompt templates are used via **PromptTemplate.from_template()** or **ChatPromptTemplate.from_message()**.  **The PromptTemplate** is primarily used for general text-based models, while **ChatPromptTemplate** is mainly used for chat-based models.  
In this post, I will explain using the [gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) model registered on Hugging Face, leveraging the `ChatBrainAI()` class I previously created.

<!--
이전 글들은 단지 PromptTemplate.from_template()만을 가지고 설명했었다. 
프롬프트 템플릿은 PromptTemplate.from_template() or ChatPromptTemplate.from_message() 사용합니다. 일반 텍스트 기반 모델에 주로 사용되는 템플릿은 PrompTemplate이고, 채팅 기반 모델에 주로 사용되는 템플릿은 ChatPromptTemplate 이다. 
이번 글에서는 앞서 내가 만든 ChatBrainAI() 클래스를 활용해서, 허깅페이스에 등록된 [gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) 모델을 가지고 설명하려합니다.
-->

## *Understanding PromptTemplate*

It is a template primarily used for general text-based models. There are two ways to use PromptTemplate. I prefer **PromptTemplate.from_template()**. I will explain using only **PromptTemplate.from_template()**.
1. Using a PromptTemplate.from_template()
2. Creating a PromptTemplate object
 
<!--
일반 텍스트 기반 모델에 주로 사용되는 템플릿입니다. PrompTemplate 사용법은 아래 두가지 방법이 있습니다. 저는 PromptTemplate.from_template() 선호합니다. PromptTemplate.from_template() 로만 설명할께요.
- PromptTemplate.from_template()
- PromptTemplate 객체 생성
-->

~~~python
from ai.local.langchain_brainai import ChatBrainAI, stream_response, invoke_response
from langchain_core.prompts import PromptTemplate

llm = ChatBrainAI('gemma-2-2b-it')

template = """<start_of_turn>system
You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
<end_of_turn>
<start_of_turn>user
{question}
<end_of_turn>
<start_of_turn>model
"""

prompt = PromptTemplate.from_template(
    template
) 

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, "<start_of_turn>model")
~~~

```
The capital of the United States is Washington, D.C.
```

## *Understanding ChatPromptTemplate*

It is a template primarily used for chat-based conversational models, with roles categorized as `system`, `user`, and `assistant`. In particular, prompts based on conversational content are mainly implemented using **ChatPromptTemplate.from_message()**.

<!--
채팅 기반 대화형 모델에 주요 사용되는 템플릿입니다. Role 구분(system, user, assistant)되어 있습니다. 특히 대화 내용을 기반으로 한 Prompt 구현은 ChatPromptTemplate.from_message() 로 주로 합니다. 
-->

~~~python
from ai.local.langchain_brainai import ChatBrainAI, stream_response, invoke_response
from langchain_core.prompts import ChatPromptTemplate

llm = ChatBrainAI('gemma-2-2b-it')

template =  [
    ("system", "You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly."),
    ("human", "{question}\nAnswer:")
]

prompt = ChatPromptTemplate.from_messages(
    template
)

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, "<start_of_turn>model")
~~~

```
The capital of the United States is Washington, D.C.
```