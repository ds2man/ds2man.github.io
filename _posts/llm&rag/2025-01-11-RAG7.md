---
title: Learning how to use prompt files with ChatOllama.
description: Let's load a prompt files and build LangChain with ChatOllama.
author: DS2Man
date: 2025-01-11 11:00:00 +0000
categories: [LLM&RAG, L&R-Understanding]
tags:
  - RAG
  - ChatOllama
  - LangChain
  - LCEL
  - Prompt
math: true
pin: true
---

In the previous post, I used `ChatOllama()` to apply various general prompts and conversational prompts to configure LangChain. Since prompts can be reused in various roles in the future, they should be managed as prompt files for better utilization. In this post, I will share examples using the **load_prompt()** method and the custom **load_chat_prompt()** method I created.

<!--
이전 글에서는 ChatOllama()를 활용해서 다양한 일반형 Prompt와 대화형 Prompt를 적용, LangChain을 구성했었다.
Prompt는 앞으로 역활에 따라 다양하게 재사용될 수 있기 때문에, Prompt file로 관리하여, 활용해야 할것이다.
이번글에서는 load_prompt() 메서드와 내가 만든 load_chat_prompt() 매서드를 활용한 예제를 공유하겠다.
-->

## *Directory*

~~~bash
└── ai 
 ├── local
 │ └── langchain_brainai.py 
 │  ├── ChatBrainAI(AIModelBase)
 │  ├── stream_response()
 │  ├── invoke_response()
 │  └── load_chat_prompt()
 └── aibase.py
   └── AIModelBase(metaclass=ABCMeta)
~~~

## *Applying PromptTemplate Files*

- Testing Model : gemma2:latest, llama3.2-vision:latest, llava:7b, gemma:7b, deepseek-r1:14b

~~~python
from langchain_ollama import ChatOllama
from langchain_core.prompts import load_prompt
from ai.local.langchain_brainai import stream_response, invoke_response
from langchain_core.prompts import PromptTemplate

selected_model = "gemma2:latest"
selected_model_catogory = "OpenAI&Ollama"
llm = ChatOllama(model=selected_model)

prompt = load_prompt(f"prompts/{selected_model_catogory}/basic.yaml")
# [basic.yaml]
# _type: "prompt"
# template: |
#   #System:
#   You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly.
#
#   #Question: 
#   {question}
#  
#   #Answer:
# input_variables: ["question"]

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question":question})
# invoke_response(response, "")
~~~

```
The capital of the United States is Washington, D.C.
```

## *Applying ChatPromptTemplate Files*

- Testing Model : gemma2:latest, llama3.2-vision:latest, llava:7b, gemma:7b, deepseek-r1:14b

~~~python
from langchain_core.prompts import ChatPromptTemplate

def load_chat_prompt(file_path, encoding="utf-8"):
    with open(file_path, "r", encoding=encoding) as f:
        prompt_messages = [(item["role"], item["message"]) for item in json.load(f)]

        prompt = ChatPromptTemplate.from_messages(prompt_messages)

        return prompt
~~~

~~~python
from langchain_ollama import ChatOllama
from ai.local.langchain_brainai import stream_response, invoke_response, load_chat_prompt

selected_model = "gemma2:latest"
selected_model_catogory = "OpenAI&Ollama"
llm = ChatOllama(model=selected_model)

prompt = load_chat_prompt(f"prompts/{selected_model_catogory}/basic_chat.json")
# [basic_chat.json]
# [
#   {"role": "system", "message": "You are a friendly AI assistant. Your name is DS2Man. Please answer questions briefly."},
#   {"role": "human", "message": "{question}"}
# ]

chain = prompt | llm

question = "What is the capital of the United States?"
response = chain.stream({"question": question})
stream_response(response)

# response = chain.invoke({"question": question})
# invoke_response(response, "")
~~~

```
The capital of the United States is Washington, D.C.
```